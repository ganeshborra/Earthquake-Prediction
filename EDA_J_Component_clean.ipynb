{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOkt9XyUN9xX"
   },
   "source": [
    "##Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "I1OqoLg1fIoy",
    "outputId": "53c03854-2716-4dee-d05d-b1ecc3e0a6f4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import data_table\n",
    "data_table.enable_dataframe_formatter()\n",
    "\n",
    "# Read CSV file with space delimiter\n",
    "df = pd.read_csv('/content/Earthquake_Data.csv', delimiter=r'\\s+')\n",
    "\n",
    "# Print the first 5 rows of the data frame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neBlKhMPOCsx"
   },
   "source": [
    "##Preprocessing\n",
    "No preprocessing required because the data is already clean and structured.\n",
    "We just have to change the column names to meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "lgmzSvsh2utY",
    "outputId": "00c20ee9-b59a-4461-a332-1629f44cc5c9"
   },
   "outputs": [],
   "source": [
    "new_column_names = [\"Date(YYYY/MM/DD)\",  \"Time(UTC)\", \"Latitude(deg)\", \"Longitude(deg)\", \"Depth(km)\", \"Magnitude(ergs)\",\n",
    "                    \"Magnitude_type\", \"No_of_Stations\", \"Gap\", \"Close\", \"RMS\", \"SRC\", \"EventID\"]\n",
    "\n",
    "df.columns = new_column_names\n",
    "ts = pd.to_datetime(df[\"Date(YYYY/MM/DD)\"] + \" \" + df[\"Time(UTC)\"])\n",
    "df = df.drop([\"Date(YYYY/MM/DD)\", \"Time(UTC)\"], axis=1)\n",
    "df.index = ts\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auO8-XmcZJww",
    "outputId": "9f3314ba-33ef-4b76-aa6d-dac0bb19845f"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxrCWKaROezl"
   },
   "source": [
    "##Export Preprocessed dataset\n",
    "Export the data into xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3aaOJaN5PKJ",
    "outputId": "4bf665cf-30b6-403a-f0d7-f6ca22feb068"
   },
   "outputs": [],
   "source": [
    "file_name = 'Earthquake_data_processed.xlsx'\n",
    "\n",
    "# saving the excel\n",
    "df.to_excel(file_name)\n",
    "print('DataFrame is written to Excel File successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_aozrbCnAIr"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u0kENybOpIl"
   },
   "source": [
    "##Partition the data into Training and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIBEitgJw4_w"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select relevant columns\n",
    "X = df[['Latitude(deg)', 'Longitude(deg)', 'Depth(km)', 'No_of_Stations']]\n",
    "y = df['Magnitude(ergs)']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igl8hYNzOz1v"
   },
   "source": [
    "##Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM6Xo26cO5H2"
   },
   "source": [
    "<h3>Loading the model and fitting it with training data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "lRLvOXsj0NkJ",
    "outputId": "2622c70c-0f21-46fe-dfa0-7199b91bd86b"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train the linear regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSAvemGCPINj"
   },
   "source": [
    "<h3>Predict the testing data</h3>\n",
    "Find the predicted values and evaluate it using metrics of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7Q9XWSe0Qzw",
    "outputId": "187f99e5-19fd-4c0b-ae65-a992250e6ed9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "scores= {\"Model name\": [\"Linear regression\", \"SVM\", \"Random Forest\"], \"mse\": [], \"R^2\": []}\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Compute R^2 and MSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "scores['mse'].append(mse)\n",
    "scores['R^2'].append(r2)\n",
    "\n",
    "print(\"R^2: {:.2f}, MSE: {:.2f}\".format(r2, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpmmAaP3Pi9v"
   },
   "source": [
    "<h3>Predict for new data</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lvjfbKI0Sio",
    "outputId": "77205e61-cf8e-43b8-f402-c114a1a0fe67"
   },
   "outputs": [],
   "source": [
    "# Predict on new data\n",
    "new_data = [[33.89, -118.40, 16.17, 11], [37.77, -122.42, 8.05, 14]]\n",
    "new_pred = regressor.predict(new_data)\n",
    "print(\"New predictions:\", new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcFg2ZcgR1gg"
   },
   "source": [
    "<h3>Plot multiple linear regression model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "tML0xmpD7gJx",
    "outputId": "989dc6e4-f867-4e34-da01-168e0e6cd24f"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the regression line\n",
    "sns.regplot(x=X_test['Latitude(deg)'], y=y_test, color='blue', scatter_kws={'s': 10})\n",
    "sns.regplot(x=X_test['Longitude(deg)'], y=y_test, color='red', scatter_kws={'s': 10})\n",
    "sns.regplot(x=X_test['Depth(km)'], y=y_test, color='yellow', scatter_kws={'s': 10})\n",
    "sns.regplot(x=X_test['No_of_Stations'], y=y_test, color='violet', scatter_kws={'s': 10})\n",
    "plt.legend(labels=['Latitude(deg)', 'Longitude(deg)', 'Depth(km)', 'No_of_Stations'])\n",
    "plt.xlabel('Predictor Variables')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Multiple Linear Regression Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TylNzFetSFpn"
   },
   "source": [
    "##SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bW6rmsMESFpo"
   },
   "source": [
    "<h3>Loading the model and fitting it with training data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8hcAPPsIgUq",
    "outputId": "bffb94ba-f217-496a-8a34-510d857828c5"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Select a subset of the training data\n",
    "subset_size = 500\n",
    "X_train_subset = X_train[:subset_size]\n",
    "y_train_subset = y_train[:subset_size]\n",
    "\n",
    "# Create an SVM model\n",
    "svm = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "\n",
    "# Train the SVM model on the subset of data\n",
    "svm.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = svm.score(X_test, y_test)\n",
    "print(\"Test score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp9VJzBcSFpo"
   },
   "source": [
    "<h3>Predict the testing data</h3>\n",
    "Find the predicted values and evaluate it using metrics like MSE, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4pRS7pRSYN1",
    "outputId": "d8f12b74-f687-4af4-c3a0-89571eca3ee6"
   },
   "outputs": [],
   "source": [
    "# Predict on the testing set\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Compute R^2 and MSE\n",
    "r2_svm = r2_score(y_test, y_pred_svm)\n",
    "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
    "\n",
    "scores['mse'].append(mse_svm)\n",
    "scores['R^2'].append(r2_svm)\n",
    "\n",
    "print(\"SVM R^2: {:.2f}, MSE: {:.2f}\".format(r2_svm, mse_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlGlzxWISFpp"
   },
   "source": [
    "<h3>Predict for new data</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H75rLzHiSZMM",
    "outputId": "fb5d7640-e173-4391-af52-31c6448ccbec"
   },
   "outputs": [],
   "source": [
    "# Predict on new data\n",
    "new_pred_svm = svm.predict(new_data)\n",
    "print(\"New SVM predictions:\", new_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI2Zm48sSFpq"
   },
   "source": [
    "<h3>Plot model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "CUkGRpvpSakm",
    "outputId": "74d661a1-7d86-4b0f-cdf0-15e415a530be"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "# create mesh grids\n",
    "def make_meshgrid(x, y, h =.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "# plot the contours\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "# color = ['y', 'b', 'g', 'k']\n",
    "\n",
    "subset_size = 500\n",
    "\n",
    "# modify the column names based on the dataset\n",
    "features = df[['Magnitude(ergs)','Latitude(deg)']][:subset_size].values\n",
    "classes = df['Magnitude_type'][:subset_size].values\n",
    "\n",
    "# create 3 svm with rbf kernels\n",
    "svm1 = SVC(kernel ='rbf')\n",
    "svm2 = SVC(kernel ='rbf')\n",
    "svm3 = SVC(kernel ='rbf')\n",
    "svm4 = SVC(kernel ='rbf')\n",
    "\n",
    "# fit each svm's\n",
    "svm1.fit(features, (classes=='ML').astype(int))\n",
    "svm2.fit(features, (classes=='Mx').astype(int))\n",
    "svm3.fit(features, (classes=='Md').astype(int))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "X0, X1 = features[:, 0], features[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "# plot the contours\n",
    "'''\n",
    "plot_contours(ax, svm1, xx, yy, cmap = plt.get_cmap('hot'), alpha = 0.8)\n",
    "plot_contours(ax, svm2, xx, yy, cmap = plt.get_cmap('hot'), alpha = 0.3)\n",
    "plot_contours(ax, svm3, xx, yy, cmap = plt.get_cmap('hot'), alpha = 0.5)\n",
    "'''\n",
    "color = ['y', 'b', 'g', 'k', 'm']\n",
    "\n",
    "\n",
    "\n",
    "for i in range(subset_size):\n",
    "    if classes[i] == 'ML':\n",
    "        plt.scatter(features[i][0], features[i][1], s = 20, c = color[0])\n",
    "    elif classes[i] == 'Mx':\n",
    "        plt.scatter(features[i][0], features[i][1], s = 20, c = color[1])\n",
    "    elif classes[i] == 'Md':\n",
    "        plt.scatter(features[i][0], features[i][1], s = 20, c = color[2])\n",
    "    else:\n",
    "        plt.scatter(features[i][0], features[i][1], s = 20, c = color[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS33P69XWJG0",
    "outputId": "47fb4cdd-c241-479e-9e98-6d32b3c7ab4b"
   },
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df['Magnitude_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRVcPQ7LSFw1"
   },
   "source": [
    "##Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aR2EgvAZpe3"
   },
   "source": [
    "\n",
    "\n",
    ">**Note: Naive bayes is used for strings and numbers(categorically) it can be used for classification so it can be either 1 or 0 nothing in between like 0.5 (regression). Even if we force naive bayes and tweak it a little bit for regression the result is disappointing; A team experimented with this and achieve not so good results.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef7Oi2pMYOB3"
   },
   "source": [
    "**`This code is just for predicting categorical data magnitude type with Naive Bayes`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "-XEp7eIid6as",
    "outputId": "dc06c5c8-7745-4536-cf76-91d2fcaa6d60"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read CSV file with space delimiter\n",
    "df = pd.read_csv('/content/Earthquake_Data.csv', delimiter=r'\\s+')\n",
    "\n",
    "new_column_names = [\"Date(YYYY/MM/DD)\",  \"Time(UTC)\", \"Latitude(deg)\", \"Longitude(deg)\", \"Depth(km)\", \"Magnitude\",\n",
    "                    \"Magnitude_Category\", \"No_of_Stations\", \"Gap\", \"Close\", \"RMS\", \"SRC\", \"EventID\"]\n",
    "\n",
    "df.columns = new_column_names\n",
    "\n",
    "# Convert magnitude column to categorical data\n",
    "df['Magnitude_Category'] = pd.cut(df['Magnitude'], bins=[0, 5, 6, 7, np.inf], labels=['Minor', 'Moderate', 'Strong', 'Major'])\n",
    "\n",
    "# Encode Magnitude Category\n",
    "le = LabelEncoder()\n",
    "df['Magnitude_Category_Encoded'] = le.fit_transform(df['Magnitude_Category'])\n",
    "\n",
    "# Normalize latitude and longitude values\n",
    "scaler = MinMaxScaler()\n",
    "df[['Latitude(deg)', 'Longitude(deg)']] = scaler.fit_transform(df[['Latitude(deg)', 'Longitude(deg)']])\n",
    "\n",
    "# Select features\n",
    "X = df[['Latitude(deg)', 'Longitude(deg)', 'No_of_Stations']]\n",
    "y = df['Magnitude_Category_Encoded']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Gaussian Naive Bayes model on the training data\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GEcwwNod-T6"
   },
   "outputs": [],
   "source": [
    "# Use the trained model to make predictions on the testing data\n",
    "y_pred = gnb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_TDi29xepW6",
    "outputId": "16421654-d942-4ea6-b8ec-88d87ad3519d"
   },
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Calculate and print the confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', cm)\n",
    "\n",
    "cr = classification_report(y_test, y_pred, labels=[0, 1, 2, 3], target_names=['Minor', 'Moderate', 'Strong', 'Major'])\n",
    "print('Classification Report:\\n', cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "O_egygdaEEcC",
    "outputId": "251e4edb-199e-4efb-ad68-af3d4785e4e1"
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot of actual vs predicted values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_test['Longitude(deg)'], X_test['Latitude(deg)'], c=y_test, cmap='viridis')\n",
    "plt.title('Actual Magnitude Category')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n",
    "print(\" \")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_test['Longitude(deg)'], X_test['Latitude(deg)'], c=y_pred, cmap='viridis')\n",
    "plt.title('Predicted Magnitude Category')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted Magnitude Category')\n",
    "plt.ylabel('Actual Magnitude Category')\n",
    "plt.show()\n",
    "print(\" \")\n",
    "\n",
    "cr = classification_report(y_test, y_pred, labels=[0, 1, 2, 3], target_names=['Minor', 'Moderate', 'Strong', 'Major'], output_dict=True)\n",
    "# Convert classification report dictionary to DataFrame\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "\n",
    "# Create bar plot of classification report scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=cr_df.index, y=cr_df['f1-score'])\n",
    "plt.xlabel('Magnitude Category')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score by Magnitude Category')\n",
    "plt.show()\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aycQO6kaSF5j"
   },
   "source": [
    "##Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPp-J-bgSF5k"
   },
   "source": [
    "<h3>Loading the model and fitting it with training data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "M_MktqiZSlh4",
    "outputId": "a0547aa5-42e0-4ac5-f671-07c1150e1b1f"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize a random forest regressor with 100 trees\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DuysDkWSF5k"
   },
   "source": [
    "<h3>Predict the testing data and evaluate it</h3>\n",
    "Find the predicted values and evaluate it using metrics like MSE, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHCtB632SlFP",
    "outputId": "bf7192be-3ce4-4c91-97e9-d7f794f9b4ec"
   },
   "outputs": [],
   "source": [
    "# Predict the target variable on the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model using mean squared error and R^2 score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "scores['mse'].append(mse)\n",
    "scores['R^2'].append(r2)\n",
    "\n",
    "print('Mean Squared Error: ', mse)\n",
    "print('R^2 Score: ', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IZd9PIaSF5l"
   },
   "source": [
    "<h3>Plot model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NONsjVeP2Us"
   },
   "source": [
    "**Scatter plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "-yFfVJ_DSkmb",
    "outputId": "b34552ef-c206-466e-fb27-444ab30b054c"
   },
   "outputs": [],
   "source": [
    "# Plot the predicted and actual values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Magnitude')\n",
    "plt.ylabel('Predicted Magnitude')\n",
    "plt.title('Random Forest Regression Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX3R4X52RGRi"
   },
   "source": [
    "**Feature Importance**<br>\n",
    "This plot shows the importance of each feature in the model. You can create a feature importance plot using the feature_importances_ attribute of the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "ARhbd_TlPcd0",
    "outputId": "8df7fab1-4017-45b5-912a-8c9add898fb3"
   },
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "features = ['Latitude', 'Longitude', 'Depth', 'No. of Stations']\n",
    "plt.bar(features, importances)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhVCeEeORM7j"
   },
   "source": [
    "**Residual Plot**<br>\n",
    "A residual plot shows the difference between the actual values and the predicted values. You can create a residual plot using the residplot() function from the seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "aQJiyVT8QA_C",
    "outputId": "4fcf356d-50d2-47e6-f041-e54c484a050e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.residplot(x= y_test, y =y_pred, color='orange')\n",
    "plt.xlabel('Predicted Magnitude')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0DTzbBMVN4B"
   },
   "source": [
    "**Actual vs. Predicted Line Plot**<br>\n",
    "Actual vs. Predicted Line Plot: A line plot can be used to show the trend of the actual and predicted values over time (if the data is time-series). You can create a line plot using the plot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "JqZp0IpkRCkK",
    "outputId": "094cf441-f06f-4b84-e62d-b683e538ea92"
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test.index[:20], y_test[:20], color='blue', label='Actual Magnitude')\n",
    "plt.plot(y_test.index[:20], y_pred[:20], color='orange', label='Predicted Magnitude')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Actual vs. Predicted Line Plot')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMAV-NHOaNWu"
   },
   "source": [
    "<h2>Concluding the accurate model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "id": "8OWXjp-Yaiz4",
    "outputId": "e8b62f24-5f73-4aa9-8042-fb0b464d7363"
   },
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "h8lP2h3obPPd",
    "outputId": "045c8239-eab2-4d8c-8179-bb0c83dddb9a"
   },
   "outputs": [],
   "source": [
    "scores_df[scores_df[\"mse\"] == scores_df[\"mse\"].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "ktxjNai3ciId",
    "outputId": "0c8a8659-630e-448c-c1de-ee37003a1d77"
   },
   "outputs": [],
   "source": [
    "scores_df[scores_df[\"R^2\"] == scores_df[\"R^2\"].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ts--omS8ck68"
   },
   "source": [
    "From the above result we can conclude that random forest is the most accurate model for predicting the magnitude of Earthquake compared to all other models used in this project."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
